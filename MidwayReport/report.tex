\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{subfigure,indentfirst}
% for url
\usepackage{hyperref}
% for underlined text
\usepackage[normalem]{ulem}

% use some packages for importing figures of different types
% pdfig is one for importing .pdf files.  sadly, they are not all
% compatible, so you often have to convert figures to the same type.
%\usepackage{pdffig}
\usepackage{graphicx}


% this starts the document
\begin{document}

\title{CS87 Midway Progress Report: Is There N-Body Out There?}

\author{Zach Rothenberg, Brendan Werth, Gus Burchell \\
Computer Science Department, Swarthmore College, Swarthmore, PA  19081}

\maketitle

\section{Project Schedule/Milestones}

{\it
An updated project schedule with milestones and dates annotated with
information indicating which parts you have completed, which parts you are
currently working, and which parts you have yet to start.. Be specific about
what you are going to do (or have done), what you have left to do, and in what
order you plan to do these things.}

\begin{itemize}
\item Week 1: We started by implementing the naive, sequential N-body implementation (particle-particle force calculations and lockstep integration). We also implemented a leap-frog integration function. 
On the parallel side, we implemented the particle-particle MPI algorithm. All of our implamentation thus far has been in C++. We have done limited testing, of both the sequential and MPI implementations, on a single  machine, using checking behavior through heuristics about the raw program output (ie not yet visualized).
\item Week 2: We implemented the sequential Barnes-Hut tree algorithm in C++. We ran tests of the MPI particle-particle on multiple machines and ran into issues, either due to deadlock or communication overhead (more on this below). We are continuing to investigate and debug these issues.

\item Week 3: While we initially planned to contact Andrew Danner about visualizing of our algorithm, we have decided that we will set aside visualization for now. Instead, we will consider static plotting sufficient and focus our limited time on the actual algorithm implementation and experimentation. Our first priority in this coming week is fixing our problems with the naive MPI solution. We need to sort out these troubles as soon as possible, so that we can start performing experiment runs. More specifically, we want to start by looking at general measurements of speedup, as well as the effect of spreading the load across different machines. It will be interesting to look at the effects of limiting locality through using separate machines, and to what degree (if any) the added overhead is contributing to our current issues. Once we have a working parallel implementation and can start large runs going, we will shift focus back to the algorithm and finish implementing Barnes-Hut, so we can see how this more complex algorithm performs differently when parallelized. Time permitting, we will look at the effects other Barnes-Hut extensions have on speedup.
\end{itemize}

\section {Difficulties}
{\it
One paragraph describing any difficulties you have encountered so far and how
you plan to resolve them (or how you did resolve them). If you don' t know how
to resolve them or have some ideas but have not completely figured it out yet,
then explicitly tell me this so that I can try to suggest some solutions.}

\begin{itemize}
    \item Currently there are segfaults in the sequential Barnes-Hut algorithm. This is the first sign that we have encountered suggesting that parallelizing the Barnes-Hut algorithm appears very challenging. We don't have specific questions about this yet, but may in the coming week. 
    \item As an example of a difficulty, updating the tree as particles move is nontrivial. A representative tree is needed at every time step in order to get accurate approximations of the particle positions, but ideally we would rebuild the parts of the tree that have changed. Currently we are rebuilding the entire tree every time-step which, while $O(nlogn)$ and therefore more efficient than the $O(n^2)$ naive approach, is more computationally intensive than we'd prefer. We anticipate this problem being one of great focus in the coming week, but regardless of our progress with it we plan on being able to collect valuable data about the success of our parallelization as long as we can get our sans-Barns-Hut MPI implementation running.
    \item As mentioned above, we are getting deadlock when we run our MPI implementing of particle-particle across machines. It works, however, when running multiple processes on a single machine. We think this is due to deadlocking that occurs when we distribute the load over multiple machines, but it could also just be due to the communication overhead becoming massive and an issue. We don't have any clear insight on how me might fix this issue, other than general debugging and playing with the code, so any idea you have would be appreciated.
\end{itemize}

\end{document}

